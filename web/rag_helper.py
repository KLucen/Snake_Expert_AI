import json
import os
import re
from openai import OpenAI
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class SnakeRAGGenerator:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        self.knowledge_base = self._load_knowledge_base()
        self.prompt_template = """
        è¯´æ˜ï¼šè¯·ä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
        
        ä½ æ˜¯è›‡ä¿¡æ¯åŠ©ç†ä¸“å®¶ã€‚ç»™å®šä¸€ä¸ªè›‡çš„ç‰©ç§åç§°ï¼Œè¿”å›ä¸€ä¸ªç»“æ„è‰¯å¥½ã€ç®€æ´çš„æ ‡è®°æ ¼å¼çš„è§£é‡Šï¼ŒåŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š
        1. ğŸ**å·²ç¡®å®šçš„ç‰©ç§åŠå…¶å…±åŒåç§°**
        2. â˜ ï¸**æ˜¯å¦æœ‰æ¯’**ï¼ˆæ˜¯/å¦+å±é™©çº§åˆ«ï¼‰
        3. ğŸ“**åˆ†å¸ƒåŒºåŸŸä¸ç”Ÿå¢ƒ**
        4. ğŸ§ **ç®€è¦è¡Œä¸ºæ€»ç»“**ï¼ˆ1-2è¡Œï¼‰
        5. âš ï¸**å®‰å…¨æç¤º**ï¼ˆé¡¹ç›®åˆ—è¡¨ï¼Œæœ€å¤š4ç‚¹ï¼‰
        6. ğŸš‘**å’¬ä¼¤æ€¥æ•‘**ï¼ˆ4-5å°æ­¥ï¼‰
        
        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        {context}
        
        è¯·æ ¹æ®ä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”å…³äºè›‡ç§ {species_name} çš„é—®é¢˜ï¼š
        """
        # åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.index, self.id_to_data = self._build_vector_index()

    def _load_knowledge_base(self):
        """åŠ è½½æœ¬åœ°è›‡ç±»çŸ¥è¯†åº“"""
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            json_path = os.path.join(base_dir, 'package.json')

            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # åˆ›å»ºå­¦åå’Œä¸­æ–‡åçš„æ˜ å°„
                knowledge = {}
                for item in data:
                    # æ ‡å‡†åŒ–å­¦åï¼šç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™
                    sci_name = re.sub(r'\s+', ' ', item['scientific_name']).strip().lower()
                    knowledge[sci_name] = item

                    # æ·»åŠ ä¸­æ–‡åæ˜ å°„
                    cn_name = item['chinese_name'].strip()
                    knowledge[cn_name] = item
                return knowledge
        except Exception as e:
            print(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {}

    def _build_vector_index(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())
        id_to_data = {}
        embeddings = []
        for name, data in self.knowledge_base.items():
            # å°†è›‡çš„å­¦åå’Œä¸­æ–‡åç»„åˆä½œä¸ºå‘é‡åŒ–çš„æ–‡æœ¬
            text = f"{name} {data.get('chinese_name', '')}"
            embedding = self.embedder.encode(text)
            embeddings.append(embedding)
            id_to_data[len(id_to_data)] = data
        embeddings = np.array(embeddings).astype('float32')
        index.add(embeddings)
        return index, id_to_data

    def _retrieve_context(self, species_name):
        """ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯"""
        # å°†æŸ¥è¯¢çš„è›‡ç§åç§°å‘é‡åŒ–
        query_text = species_name
        query_embedding = self.embedder.encode(query_text)
        query_embedding = np.array([query_embedding]).astype('float32')
        # åœ¨å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ
        _, indices = self.index.search(query_embedding, 1)
        index = indices[0][0]
        snake_data = self.id_to_data.get(index)
        if snake_data:
            return self._format_context(snake_data)
        return "æ²¡æœ‰æ‰¾åˆ°è¯¥è›‡ç§çš„ç›¸å…³ä¿¡æ¯"

    def _format_context(self, snake_data):
        """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = f"""
        **å­¦å**: {snake_data.get('scientific_name', 'æœªçŸ¥')}
        **ä¸­æ–‡å**: {snake_data.get('chinese_name', 'æœªçŸ¥')}
        **æ¯’æ€§**: {snake_data.get('toxicity', 'æœªçŸ¥')}
        **å±é™©çº§åˆ«**: {snake_data.get('danger_level', 'æœªçŸ¥')}
        **åˆ†å¸ƒ**: {snake_data.get('distribution', 'æœªçŸ¥')}
        **æ –æ¯åœ°**: {snake_data.get('habitat', 'æœªçŸ¥')}
        **è¡Œä¸º**: {snake_data.get('behavior', 'æœªçŸ¥')}
        """

        if 'other_info' in snake_data and snake_data['other_info']:
            context += "\n**å…¶ä»–ä¿¡æ¯**:\n- " + "\n- ".join(snake_data['other_info'])

        return context

    def get_snake_info(self, species_name):
        """ä½¿ç”¨RAGè·å–è›‡ç±»ä¿¡æ¯"""
        # 1. æ£€ç´¢ä¸Šä¸‹æ–‡
        context = self._retrieve_context(species_name)
        # 2. å¢å¼ºï¼šå°†æ£€ç´¢åˆ°çš„å†…å®¹ä¸åŸå§‹é—®é¢˜æ‹¼æ¥
        prompt = self.prompt_template.format(
            context=context,
            species_name=species_name
        )
        # 3. ç”Ÿæˆï¼šLLMåŸºäºå¢å¼ºåçš„promptç”Ÿæˆæœ€ç»ˆå›ç­”
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªè›‡ç±»ä¿¡æ¯ä¸“å®¶ï¼Œä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,  # é™ä½éšæœºæ€§
                max_tokens=500,
                stream=False
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"APIè°ƒç”¨é”™è¯¯: {e}")
            return f"# é”™è¯¯\næ— æ³•è·å–{species_name}çš„ä¿¡æ¯ï¼Œè¯·ç¨åå†è¯•ã€‚"